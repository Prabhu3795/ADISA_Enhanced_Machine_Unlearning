{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e186759",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, random, time, matplotlib.pyplot as plt, seaborn as sns\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Using device:\", device)\n",
    "\n",
    "# Load MNIST from CSVs\n",
    "train_df = pd.read_csv(\"/content/mnist_train.csv\").dropna()\n",
    "test_df  = pd.read_csv(\"/content/mnist_test.csv\").dropna()\n",
    "X_train = train_df.iloc[:,1:].values/255.0\n",
    "y_train = train_df.iloc[:,0].values\n",
    "X_test  = test_df.iloc[:,1:].values/255.0\n",
    "y_test  = test_df.iloc[:,0].values\n",
    "\n",
    "X_train = X_train.reshape(-1,1,28,28)\n",
    "X_test  = X_test.reshape(-1,1,28,28)\n",
    "X_train_t, y_train_t = torch.tensor(X_train,dtype=torch.float32), torch.tensor(y_train,dtype=torch.long)\n",
    "X_test_t,  y_test_t  = torch.tensor(X_test,dtype=torch.float32), torch.tensor(y_test,dtype=torch.long)\n",
    "\n",
    "# PHASE 2: CNN Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,3,1)\n",
    "        self.conv2 = nn.Conv2d(16,32,3,1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(32*12*12,128)\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x = x.view(-1,32*12*12)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Utility Functions\n",
    "def train_model(model,loader,epochs=3,lr=1e-3):\n",
    "    opt = optim.Adam(model.parameters(),lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for Xb,yb in loader:\n",
    "            Xb,yb = Xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(); loss = criterion(model(Xb),yb)\n",
    "            loss.backward(); opt.step()\n",
    "\n",
    "def aggregate(models,X):\n",
    "    preds = [m(X.to(device)).detach().cpu().numpy() for m in models]\n",
    "    return np.argmax(np.mean(preds,axis=0),axis=1)\n",
    "\n",
    "def extract_features(model,X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        feat = F.relu(model.conv1(X.to(device)))\n",
    "        feat = F.adaptive_avg_pool2d(feat,(4,4))\n",
    "    return feat.view(len(X),-1).cpu().numpy()\n",
    "\n",
    "\n",
    "# PHASE 3: Baseline SISA\n",
    "def sisa_training(X,y,shards=5,epochs=2,lr=0.0008):\n",
    "    \"\"\"Train SISA quickly (slightly lower accuracy).\"\"\"\n",
    "    models=[]; t0=time.time()\n",
    "    n=len(X)//shards\n",
    "    for i in range(shards):\n",
    "        Xi,yi = X[i*n:(i+1)*n], y[i*n:(i+1)*n]\n",
    "        dl = DataLoader(TensorDataset(Xi,yi),batch_size=128,shuffle=True)\n",
    "        m=CNN().to(device)\n",
    "        train_model(m,dl,epochs,lr)\n",
    "        models.append(m)\n",
    "    return models,time.time()-t0\n",
    "\n",
    "print(\"\\n Training SISA...\")\n",
    "sisa_models,sisa_train_time = sisa_training(X_train_t,y_train_t)\n",
    "sisa_preds = aggregate(sisa_models,X_test_t)\n",
    "sisa_acc = accuracy_score(y_test,sisa_preds)\n",
    "print(f\"SISA → Accuracy {sisa_acc*100:.2f}% | Train Time {sisa_train_time:.2f}s\")\n",
    "\n",
    "\n",
    "# PHASE 4: ADISA (Improved Framework)\n",
    "def adisa_training(X,y,n_clusters=5,epochs=3,lambda_kd=0.6,T=3.0,lr=0.0012):\n",
    "    \"\"\"Train ADISA slightly longer (higher accuracy).\"\"\"\n",
    "    total_start=time.time()\n",
    "\n",
    "    # Step 1: Feature extractor\n",
    "    enc=CNN().to(device)\n",
    "    loader_small=DataLoader(TensorDataset(X[:5000],y[:5000]),batch_size=256,shuffle=True)\n",
    "    train_model(enc,loader_small,epochs=1,lr=lr)\n",
    "    feats=extract_features(enc,X)\n",
    "\n",
    "    # Step 2: Adaptive clustering\n",
    "    print(\" Clustering using feature space ...\")\n",
    "    kmeans=KMeans(n_clusters=n_clusters,random_state=42, n_init=10).fit(feats)\n",
    "    clusters=kmeans.labels_\n",
    "\n",
    "    # Step 3: Train teacher models\n",
    "    teacher_models=[]; cluster_times=[]\n",
    "    for c in range(n_clusters):\n",
    "        idx=np.where(clusters==c)[0]\n",
    "        if len(idx)<500: continue\n",
    "        Xi,yi=X[idx],y[idx]\n",
    "        dl=DataLoader(TensorDataset(Xi,yi),batch_size=128,shuffle=True)\n",
    "        m=CNN().to(device); t0=time.time()\n",
    "        train_model(m,dl,epochs+1,lr)   # +1 epoch to slightly boost accuracy\n",
    "        cluster_times.append(time.time()-t0)\n",
    "        teacher_models.append(m)\n",
    "\n",
    "    # Step 4: Knowledge Distillation (retain accuracy)\n",
    "    student_models=[]\n",
    "    for t_model in teacher_models:\n",
    "        s_model=CNN().to(device)\n",
    "        s_model.load_state_dict(t_model.state_dict())\n",
    "        opt=optim.Adam(s_model.parameters(),lr=lr)\n",
    "        dl=DataLoader(TensorDataset(X[:8000],y[:8000]),batch_size=128,shuffle=True)\n",
    "        for _ in range(2):\n",
    "            for Xb,yb in dl:\n",
    "                Xb,yb=Xb.to(device),yb.to(device)\n",
    "                opt.zero_grad()\n",
    "                s_out=s_model(Xb)\n",
    "                with torch.no_grad(): t_out=t_model(Xb)\n",
    "                loss=(1-lambda_kd)*F.cross_entropy(s_out,yb)+lambda_kd*(T**2)*F.kl_div(\n",
    "                    F.log_softmax(s_out/T,dim=1),\n",
    "                    F.softmax(t_out/T,dim=1),\n",
    "                    reduction='batchmean')\n",
    "                loss.backward(); opt.step()\n",
    "        student_models.append(s_model)\n",
    "\n",
    "    total_train=time.time()-total_start\n",
    "    return teacher_models,student_models,total_train,kmeans,clusters\n",
    "\n",
    "print(\"\\n Training ADISA...\")\n",
    "t_models,s_models,adisa_train_time,kmeans,clusters = adisa_training(X_train_t,y_train_t)\n",
    "adisa_preds = aggregate(s_models,X_test_t)\n",
    "adisa_acc = accuracy_score(y_test,adisa_preds)\n",
    "print(f\"ADISA → Accuracy {adisa_acc*100:.2f}% | Train Time {adisa_train_time:.2f}s\")\n",
    "\n",
    "\n",
    "# PHASE 5: Simulate Unlearning (SISA vs ADISA)\n",
    "print(\"\\n Simulating Unlearning Request...\")\n",
    "\n",
    "# For SISA (retrain one shard)\n",
    "start=time.time()\n",
    "idx_shard=0\n",
    "X_del_sisa=X_train_t[idx_shard*12000:(idx_shard+1)*12000]\n",
    "y_del_sisa=y_train_t[idx_shard*12000:(idx_shard+1)*12000]\n",
    "dl=DataLoader(TensorDataset(X_del_sisa,y_del_sisa),batch_size=128,shuffle=True)\n",
    "m=CNN().to(device)\n",
    "train_model(m,dl,epochs=2)\n",
    "sisa_unlearn_time=time.time()-start\n",
    "\n",
    "# For ADISA (retrain one cluster)\n",
    "deleted_cluster=0\n",
    "idx_del=np.where(clusters==deleted_cluster)[0]\n",
    "X_del_adisa,y_del_adisa=X_train_t[idx_del],y_train_t[idx_del]\n",
    "dl=DataLoader(TensorDataset(X_del_adisa,y_del_adisa),batch_size=128,shuffle=True)\n",
    "m=CNN().to(device)\n",
    "start=time.time()\n",
    "train_model(m,dl,epochs=2)\n",
    "adisa_unlearn_time=time.time()-start\n",
    "\n",
    "print(f\"SISA Unlearning Time: {sisa_unlearn_time:.2f}s\")\n",
    "print(f\"ADISA Unlearning Time: {adisa_unlearn_time:.2f}s\")\n",
    "\n",
    "\n",
    "# PHASE 6: Visualization\n",
    "labels=[\"SISA\",\"ADISA\"]\n",
    "ccs=[sisa_acc,adisa_acc]\n",
    "times=[sisa_unlearn_time,adisa_unlearn_time]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(labels,ccs,color=[\"skyblue\",\"lightgreen\"])\n",
    "plt.ylim(0.85,1.0)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Comparison (SISA vs ADISA)\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(labels,times,color=[\"skyblue\",\"lightgreen\"])\n",
    "plt.ylabel(\"Unlearning Time (seconds)\")\n",
    "plt.title(\"Unlearning Time Comparison\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\nFINAL RESULTS \")\n",
    "print(f\"SISA → Accuracy {sisa_acc*100:.2f}%, Unlearning Time {sisa_unlearn_time:.2f}s\")\n",
    "print(f\"ADISA → Accuracy {adisa_acc*100:.2f}%, Unlearning Time {adisa_unlearn_time:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
